# 如何解决数据倾斜问题

## 提高shuffle操作的并行度
设置如下参数：
```
spark.sql.shuffle.partitions
```
提高并行度，可以让原本分配给一个task的多个不同key分配给不同的task，但极端情况下某个key数量特别多时依旧存在数据倾斜问题。
## 局部聚合 + 全局聚合
使用groupby进行分组聚合时，比较适用这种方案。
### 概述
通过对key添加随机前缀的方式，将相同的key变成不同的key，从而可以让原本被一个task处理的数据分散到多个task上，进而解决单个task处理数据量过多的问题。
### 局部聚合
- 给每个key打上一个随机数，这样原先一样的key就变成不一样了
- 对打上随机数后的数据，执行reduceByKey等聚合操作，得到局部聚合结果
### 全局聚合
- 将各个key的前缀去掉，再进行一次全局聚合，得到最终结果
## 参考文章
- https://tech.meituan.com/2016/05/12/spark-tuning-pro.html
